# Calculate format=diff
This is BLD-3.10.0 for Linux kernel version 3.10.0. No functionality changes since previous release,
just ported for new kernel version. There are few function movement on fair.c, which is an attempt to
move unused function under only when !BLD and to reduce the #ifdef.

Thanks,
Rakib


Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
---

diff --git a/init/Kconfig b/init/Kconfig
index 2d9b831..1595d2f 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -36,6 +36,14 @@ config BROKEN_ON_SMP
 	depends on BROKEN || !SMP
 	default y
 
+config BLD
+	bool "An alternate CPU load distribution technique for kernel scheduler"
+	depends on SMP
+	default y
+	help
+	  This is an alternate CPU load distribution technique based on Barbershop
+	  Load Distribution algorithm.
+
 config INIT_ENV_ARG_LIMIT
 	int
 	default 32 if !UML
diff --git a/kernel/sched/bld.h b/kernel/sched/bld.h
new file mode 100644
index 0000000..a3cb3c1
--- /dev/null
+++ b/kernel/sched/bld.h
@@ -0,0 +1,117 @@
+#ifdef CONFIG_BLD
+
+static DEFINE_RWLOCK(disp_list_lock);
+static LIST_HEAD(rq_head);
+
+static int select_cpu_for_wakeup(struct task_struct *p, int sd_flags, int wake_flags, int task_type)
+{
+	int cpu = smp_processor_id(), prev_cpu = task_cpu(p), i;
+	unsigned long load, min_load = ULONG_MAX;
+	struct cpumask *mask;
+	struct rq *rq;
+
+	if (wake_flags & WF_SYNC) {
+		if (cpu == prev_cpu)
+			return cpu;
+		mask = sched_group_cpus(cpu_rq(prev_cpu)->sd->groups);
+	} else
+		mask = sched_domain_span(cpu_rq(prev_cpu)->sd);
+
+	if (task_type) {
+		for_each_cpu(i, mask) {
+			rq = cpu_rq(i);
+			load = rq->cfs.load.weight;
+			if (load < min_load) {
+				min_load = load;
+				cpu = i;
+			}
+		}
+	} else {
+		for_each_cpu(i, mask) {
+			rq = cpu_rq(i);
+			load = rq->rt.rt_nr_running;
+			if (load < min_load) {
+				min_load = load;
+				cpu = i;
+			}
+		}
+	}
+	return cpu;
+}
+
+static int bld_select_task_rq(struct task_struct *p, int sd_flags, int wake_flags)
+{
+	struct rq *rq;
+	unsigned int cpu = smp_processor_id();
+
+	if (&p->cpus_allowed) {
+		struct cpumask *taskmask;
+		unsigned long min_load = ULONG_MAX, load, i;
+		taskmask = tsk_cpus_allowed(p);
+		for_each_cpu(i, taskmask) {
+			load = cpu_rq(i)->load.weight;
+			if (load < min_load) {
+				min_load = load;
+				cpu = i;
+			}
+		}
+	} else	if (sd_flags & SD_BALANCE_WAKE) {
+		if (!rt_task(p))
+			cpu = select_cpu_for_wakeup(p, sd_flags, wake_flags, 1);
+		else
+			cpu = select_cpu_for_wakeup(p, sd_flags, wake_flags, 0);
+		return cpu;
+	} else {
+		read_lock_irq(&disp_list_lock);
+		list_for_each_entry(rq, &rq_head, disp_load_balance) {
+			cpu = cpu_of(rq);
+			if (cpu_online(cpu))
+				break;
+		}
+		read_unlock_irq(&disp_list_lock);
+	}
+	return cpu;
+}
+
+static void bld_track_load_activate(struct rq *rq)
+{
+	unsigned long flag;
+
+	if (rq->pos != 2) {	/* if rq isn't the last one */
+		struct rq *last;
+		last = list_entry(rq_head.prev, struct rq, disp_load_balance);
+		if (rq->load.weight > last->load.weight) {
+			write_lock_irqsave(&disp_list_lock, flag);
+			list_del(&rq->disp_load_balance);
+			list_add_tail(&rq->disp_load_balance, &rq_head);
+			rq->pos = 2; last->pos = 1;
+			write_unlock_irqrestore(&disp_list_lock, flag);
+		}
+	}
+}
+
+static void bld_track_load_deactivate(struct rq *rq)
+{
+	unsigned long flag;
+
+	if (rq->pos != 0) { /* If rq isn't first one */
+		struct rq *first;
+		first = list_first_entry(&rq_head, struct rq, disp_load_balance);
+		if (rq->load.weight <= first->load.weight) {
+			write_lock_irqsave(&disp_list_lock, flag);
+			list_del(&rq->disp_load_balance);
+			list_add(&rq->disp_load_balance, &rq_head);
+			rq->pos = 0; first->pos = 1;
+			write_unlock_irqrestore(&disp_list_lock, flag);
+		}
+	}
+}
+#else
+static inline void bld_track_load_activate(struct rq *rq)
+{
+}
+
+static inline void bld_track_load_deactivate(struct rq *rq)
+{
+}
+#endif /* CONFIG_BLD */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index e8b3350..df29112 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -24,6 +24,8 @@
  *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri
  *  2007-11-29  RT balancing improvements by Steven Rostedt, Gregory Haskins,
  *              Thomas Gleixner, Mike Kravetz
+ *  2012-Feb	The Barbershop Load Distribution (BLD) algorithm - an alternate
+ *		CPU load distribution technique for kernel scheduler by Rakib Mullick.
  */
 
 #include <linux/mm.h>
@@ -85,6 +87,7 @@
 #include "sched.h"
 #include "../workqueue_internal.h"
 #include "../smpboot.h"
+#include "bld.h"
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
@@ -559,6 +562,7 @@ int get_nohz_timer_target(void)
 	int i;
 	struct sched_domain *sd;
 
+#ifndef	CONFIG_BLD
 	rcu_read_lock();
 	for_each_domain(cpu, sd) {
 		for_each_cpu(i, sched_domain_span(sd)) {
@@ -570,8 +574,10 @@ int get_nohz_timer_target(void)
 	}
 unlock:
 	rcu_read_unlock();
+#endif
 	return cpu;
 }
+
 /*
  * When add_timer_on() enqueues a timer into the timer wheel of an
  * idle CPU then this timer might expire before the next timer event
@@ -584,6 +590,7 @@ unlock:
  */
 static void wake_up_idle_cpu(int cpu)
 {
+#ifndef	CONFIG_BLD
 	struct rq *rq = cpu_rq(cpu);
 
 	if (cpu == smp_processor_id())
@@ -610,6 +617,7 @@ static void wake_up_idle_cpu(int cpu)
 	smp_mb();
 	if (!tsk_is_polling(rq->idle))
 		smp_send_reschedule(cpu);
+#endif
 }
 
 static bool wake_up_full_nohz_cpu(int cpu)
@@ -767,6 +775,7 @@ static void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 	update_rq_clock(rq);
 	sched_info_queued(p);
 	p->sched_class->enqueue_task(rq, p, flags);
+	bld_track_load_activate(rq);
 }
 
 static void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
@@ -774,6 +783,7 @@ static void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 	update_rq_clock(rq);
 	sched_info_dequeued(p);
 	p->sched_class->dequeue_task(rq, p, flags);
+	bld_track_load_deactivate(rq);
 }
 
 void activate_task(struct rq *rq, struct task_struct *p, int flags)
@@ -1249,8 +1259,12 @@ out:
 static inline
 int select_task_rq(struct task_struct *p, int sd_flags, int wake_flags)
 {
-	int cpu = p->sched_class->select_task_rq(p, sd_flags, wake_flags);
-
+	int cpu;
+#ifndef CONFIG_BLD
+	cpu = p->sched_class->select_task_rq(p, sd_flags, wake_flags);
+#else
+	cpu = bld_select_task_rq(p, sd_flags, wake_flags);
+#endif
 	/*
 	 * In order not to call set_task_cpu() on a blocking task we need
 	 * to rely on ttwu() to place the task on a valid ->cpus_allowed
@@ -1405,9 +1419,13 @@ static void sched_ttwu_pending(void)
 
 void scheduler_ipi(void)
 {
+#ifndef	CONFIG_BLD
 	if (llist_empty(&this_rq()->wake_list)
 			&& !tick_nohz_full_cpu(smp_processor_id())
 			&& !got_nohz_idle_kick())
+#else
+	if (llist_empty(&this_rq()->wake_list))
+#endif
 		return;
 
 	/*
@@ -1430,18 +1448,22 @@ void scheduler_ipi(void)
 	/*
 	 * Check if someone kicked us for doing the nohz idle load balance.
 	 */
+#ifndef	CONFIG_BLD
 	if (unlikely(got_nohz_idle_kick())) {
 		this_rq()->idle_balance = 1;
 		raise_softirq_irqoff(SCHED_SOFTIRQ);
 	}
+#endif
 	irq_exit();
 }
 
+#ifndef	CONFIG_BLD
 static void ttwu_queue_remote(struct task_struct *p, int cpu)
 {
 	if (llist_add(&p->wake_entry, &cpu_rq(cpu)->wake_list))
 		smp_send_reschedule(cpu);
 }
+#endif
 
 bool cpus_share_cache(int this_cpu, int that_cpu)
 {
@@ -1453,7 +1475,7 @@ static void ttwu_queue(struct task_struct *p, int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
 
-#if defined(CONFIG_SMP)
+#if defined(CONFIG_SMP) && !defined(CONFIG_BLD)
 	if (sched_feat(TTWU_QUEUE) && !cpus_share_cache(smp_processor_id(), cpu)) {
 		sched_clock_cpu(cpu); /* sync clocks x-cpu */
 		ttwu_queue_remote(p, cpu);
@@ -1718,7 +1740,7 @@ void sched_fork(struct task_struct *p)
 	 * Silence PROVE_RCU.
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
-	set_task_cpu(p, cpu);
+	__set_task_cpu(p, cpu);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
@@ -2651,7 +2673,11 @@ void sched_exec(void)
 	int dest_cpu;
 
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
+#ifndef CONFIG_BLD
 	dest_cpu = p->sched_class->select_task_rq(p, SD_BALANCE_EXEC, 0);
+#else
+	dest_cpu = bld_select_task_rq(p, SD_BALANCE_EXEC, 0);
+#endif
 	if (dest_cpu == smp_processor_id())
 		goto unlock;
 
@@ -2747,8 +2773,10 @@ void scheduler_tick(void)
 
 #ifdef CONFIG_SMP
 	rq->idle_balance = idle_cpu(cpu);
+#ifndef	CONFIG_BLD
 	trigger_load_balance(rq, cpu);
 #endif
+#endif
 	rq_last_tick_reset(rq);
 }
 
@@ -2994,8 +3022,10 @@ need_resched:
 
 	pre_schedule(rq, prev);
 
+#ifndef	CONFIG_BLD
 	if (unlikely(!rq->nr_running))
 		idle_balance(cpu, rq);
+#endif
 
 	put_prev_task(rq, prev);
 	next = pick_next_task(rq);
@@ -7040,6 +7070,11 @@ void __init sched_init(void)
 #endif
 		init_rq_hrtick(rq);
 		atomic_set(&rq->nr_iowait, 0);
+#ifdef CONFIG_BLD
+		INIT_LIST_HEAD(&rq->disp_load_balance);
+		list_add_tail(&rq->disp_load_balance, &rq_head);
+		rq->pos = 0;
+#endif
 	}
 
 	set_load_weight(&init_task);
@@ -7083,6 +7118,9 @@ void __init sched_init(void)
 	init_sched_fair_class();
 
 	scheduler_running = 1;
+#ifdef CONFIG_BLD
+	printk(KERN_INFO "BLD: An Alternate CPU load distributor activated.\n");
+#endif
 }
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c61a614..4233867 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2993,6 +2993,7 @@ static void task_waking_fair(struct task_struct *p)
 	se->vruntime -= min_vruntime;
 }
 
+#ifndef	CONFIG_BLD
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /*
  * effective_load() calculates the load change as seen from the root_task_group
@@ -3414,6 +3415,7 @@ unlock:
 
 	return new_cpu;
 }
+#endif	/* CONFIG_BLD */
 
 /*
  * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
@@ -5362,6 +5364,8 @@ out_unlock:
  *   needed, they will kick the idle load balancer, which then does idle
  *   load balancing for all the idle CPUs.
  */
+#ifndef	CONFIG_BLD
+
 static struct {
 	cpumask_var_t idle_cpus_mask;
 	atomic_t nr_cpus;
@@ -5406,15 +5410,6 @@ static void nohz_balancer_kick(int cpu)
 	return;
 }
 
-static inline void nohz_balance_exit_idle(int cpu)
-{
-	if (unlikely(test_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu)))) {
-		cpumask_clear_cpu(cpu, nohz.idle_cpus_mask);
-		atomic_dec(&nohz.nr_cpus);
-		clear_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
-	}
-}
-
 static inline void set_cpu_sd_state_busy(void)
 {
 	struct sched_domain *sd;
@@ -5433,6 +5428,28 @@ unlock:
 	rcu_read_unlock();
 }
 
+static inline void nohz_balance_exit_idle(int cpu)
+{
+	if (unlikely(test_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu)))) {
+		cpumask_clear_cpu(cpu, nohz.idle_cpus_mask);
+		atomic_dec(&nohz.nr_cpus);
+		clear_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
+	}
+}
+
+static int __cpuinit sched_ilb_notifier(struct notifier_block *nfb,
+					unsigned long action, void *hcpu)
+{
+	switch (action & ~CPU_TASKS_FROZEN) {
+	case CPU_DYING:
+		nohz_balance_exit_idle(smp_processor_id());
+		return NOTIFY_OK;
+	default:
+		return NOTIFY_DONE;
+	}
+}
+#endif	/* CONFIG_BLD */
+
 void set_cpu_sd_state_idle(void)
 {
 	struct sched_domain *sd;
@@ -5457,6 +5474,7 @@ unlock:
  */
 void nohz_balance_enter_idle(int cpu)
 {
+#ifndef	CONFIG_BLD
 	/*
 	 * If this cpu is going down, then nothing needs to be done.
 	 */
@@ -5469,22 +5487,9 @@ void nohz_balance_enter_idle(int cpu)
 	cpumask_set_cpu(cpu, nohz.idle_cpus_mask);
 	atomic_inc(&nohz.nr_cpus);
 	set_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
-}
-
-static int __cpuinit sched_ilb_notifier(struct notifier_block *nfb,
-					unsigned long action, void *hcpu)
-{
-	switch (action & ~CPU_TASKS_FROZEN) {
-	case CPU_DYING:
-		nohz_balance_exit_idle(smp_processor_id());
-		return NOTIFY_OK;
-	default:
-		return NOTIFY_DONE;
-	}
-}
 #endif
-
-static DEFINE_SPINLOCK(balancing);
+}
+#endif	/* CONFIG_NO_HZ_COMMON	*/
 
 /*
  * Scale the max load_balance interval with the number of CPUs in the system.
@@ -5495,6 +5500,9 @@ void update_max_interval(void)
 	max_load_balance_interval = HZ*num_online_cpus()/10;
 }
 
+#ifndef	CONFIG_BLD
+static DEFINE_SPINLOCK(balancing);
+
 /*
  * It checks each scheduling domain to see if it is due to be balanced,
  * and initiates a balancing operation if so.
@@ -5722,6 +5730,7 @@ void trigger_load_balance(struct rq *rq, int cpu)
 		nohz_balancer_kick(cpu);
 #endif
 }
+#endif	/* CONFIG_BLD */
 
 static void rq_online_fair(struct rq *rq)
 {
@@ -6145,7 +6154,9 @@ const struct sched_class fair_sched_class = {
 	.put_prev_task		= put_prev_task_fair,
 
 #ifdef CONFIG_SMP
+#ifndef	CONFIG_BLD
 	.select_task_rq		= select_task_rq_fair,
+#endif
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	.migrate_task_rq	= migrate_task_rq_fair,
 #endif
@@ -6184,6 +6195,7 @@ void print_cfs_stats(struct seq_file *m, int cpu)
 
 __init void init_sched_fair_class(void)
 {
+#ifndef	CONFIG_BLD
 #ifdef CONFIG_SMP
 	open_softirq(SCHED_SOFTIRQ, run_rebalance_domains);
 
@@ -6193,5 +6205,5 @@ __init void init_sched_fair_class(void)
 	cpu_notifier(sched_ilb_notifier, 0);
 #endif
 #endif /* SMP */
-
+#endif /* BLD */
 }
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 127a2c4..0fca6c8 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -1238,6 +1238,7 @@ static void yield_task_rt(struct rq *rq)
 #ifdef CONFIG_SMP
 static int find_lowest_rq(struct task_struct *task);
 
+#ifndef	CONFIG_BLD
 static int
 select_task_rq_rt(struct task_struct *p, int sd_flag, int flags)
 {
@@ -1295,6 +1296,7 @@ select_task_rq_rt(struct task_struct *p, int sd_flag, int flags)
 out:
 	return cpu;
 }
+#endif
 
 static void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)
 {
@@ -2066,8 +2068,9 @@ const struct sched_class rt_sched_class = {
 	.put_prev_task		= put_prev_task_rt,
 
 #ifdef CONFIG_SMP
+#ifndef	CONFIG_BLD
 	.select_task_rq		= select_task_rq_rt,
-
+#endif
 	.set_cpus_allowed       = set_cpus_allowed_rt,
 	.rq_online              = rq_online_rt,
 	.rq_offline             = rq_offline_rt,
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ce39224..98da21e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -521,6 +521,17 @@ struct rq {
 #endif
 
 	struct sched_avg avg;
+
+#ifdef CONFIG_BLD
+	struct list_head disp_load_balance;
+	/* pos indicates whether, rq is first or last
+	 * or in the middle based on load from rq_head.
+	 * 0 - First rq
+	 * 1 - stays middle
+	 * 2 - last rq
+	 */
+	char pos;
+#endif
 };
 
 static inline int cpu_of(struct rq *rq)
